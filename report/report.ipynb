{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad$ $\\qquad$$\\qquad$  **BBM409 Machine Learning: Assignmnet 2** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$ **Goal: Naive Bayes**<br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **MELTEM TOKGÖZ** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                     **21527381** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **b21527381@cs.hacettepe.edu.tr** <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PART I: Theoretical problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLE\n",
    "\n",
    "1-Solution:  <img src =\"mle1.png\">\n",
    "\n",
    "2-Solution:  <img src =\"mle2.png\">\n",
    "\n",
    "3-Solution:  <img src =\"mle3.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes\n",
    "\n",
    "1-Solution: <img src =\"nb1.png\">\n",
    "\n",
    "2-Solution: <img src =\"nb2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#********************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II: Detection of Fake News\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this part of the assignment, I will try to determine whether a headline is real or fake news.I will implement a Naive Bayes classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, I used to import the libraries I used. I did use pandas, numpy, collections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#**First-Step**Making imports******\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the nltk.corpus library for stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stopword **********************************************\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I opened the clean_fake-Train.txt file and clean_real-Train.txt file  given to me and started reading them.I read the test.csv with the pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read input file*****************************************\n",
    "fake_input = open(\"clean_fake-Train.txt\", \"r\")\n",
    "real_input = open(\"clean_real-Train.txt\",\"r\")\n",
    "test_input = pd.read_csv('test.csv', sep=',')\n",
    "test_input.columns = ['id', 'category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created the dictionaries needed for unigram and bigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#UNIGRAM AND BIGRAM BAG OF WORDS IMPLEMENTATION*******************************************************\n",
    "\n",
    "bow_all_u = {} #unigram all words dictionary\n",
    "bow_ri_u = {}  #unigram all real input words in dictionary\n",
    "bow_fi_u = {}  #unigram all fake input word in dictionary\n",
    "\n",
    "bow_all_b = {} #bigram all words dictionary\n",
    "bow_ri_b = {}  #bigram all real input words in dictionary\n",
    "bow_fi_b = {}  #bigram all fake input word in dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've also created lists of words I'll use in the next step.I also created two int values to find the line numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_line=0\n",
    "f_line=0\n",
    "\n",
    "datafake =[] #fake data all line\n",
    "datareal =[] #real data all line\n",
    "words_counter = [] #all word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I read the real and fake file line by line and split it according to the gap and added it to the required dictionaries.I've used bag of word model and I added the words I read to the dictionary as follows: {word: total occurence word in file}\n",
    "For example, x is 10 times in the real data {x: 10}.\n",
    "I added the words in Unigram one by one to the dictionary.\n",
    "I added two pairs in the bigram.I also looked at the words in the stop word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for line in fake_input.readlines():\n",
    "    f_line += 1\n",
    "    line = line.rstrip(\"\\n\")\n",
    "    datafake.append(line)\n",
    "    line = line.split(\" \")\n",
    "    #unigram dictioary\n",
    "    for words in line:\n",
    "        #if words not in stopWords:\n",
    "            words_counter.append(words)\n",
    "            x = words\n",
    "            if x not in bow_fi_u.keys():\n",
    "                bow_fi_u[x] = 1\n",
    "            else:\n",
    "                bow_fi_u[x] += 1\n",
    "            y = words\n",
    "            if y not in bow_all_u.keys():\n",
    "                bow_all_u[y] = 1\n",
    "            else:\n",
    "                bow_all_u[y] += 1\n",
    "    #bigram dictionary\n",
    "    for i in range(len(line)-1): \n",
    "        \n",
    "        z = line[i],line[i+1]\n",
    "        if z not in bow_fi_b.keys():\n",
    "            bow_fi_b[z] = 1\n",
    "        else:\n",
    "            bow_fi_b[z] += 1\n",
    "\n",
    "        t = line[i],line[i+1]\n",
    "        if  t not in bow_all_b.keys():\n",
    "            bow_all_b[t] = 1\n",
    "        else:\n",
    "            bow_all_b[t] += 1\n",
    "    \n",
    "\n",
    "for line in real_input.readlines():\n",
    "    r_line += 1\n",
    "    line = line.rstrip(\"\\n\")\n",
    "    datareal.append(line)\n",
    "    line = line.split(\" \")\n",
    "    #unigram dictionary\n",
    "    for words in line:\n",
    "        #if words not in stopWords:\n",
    "                words_counter.append(words)\n",
    "                x = words\n",
    "                if x not in bow_ri_u.keys():\n",
    "                    bow_ri_u[x] = 1\n",
    "                else:\n",
    "                    bow_ri_u[x] += 1 \n",
    "        \n",
    "                y = words\n",
    "                if y not in bow_all_u.keys():\n",
    "                    bow_all_u[y] = 1\n",
    "                else:\n",
    "                    bow_all_u[y] += 1\n",
    "    #bigram dictionary\n",
    "    for i in range(len(line)-1): \n",
    "        z = line[i],line[i+1]\n",
    "        if  z not in bow_ri_b.keys():\n",
    "            bow_ri_b[z] = 1\n",
    "        else:\n",
    "            bow_ri_b[z] += 1\n",
    " \n",
    "        t = line[i],line[i+1] \n",
    "        if  t not in bow_all_b.keys():\n",
    "            bow_all_b[t] = 1\n",
    "        else:\n",
    "            bow_all_b[t] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Fake input unique word count :  3366\n",
      "Unigram Real input unique word count :  3268\n",
      "Unigram All unique word count :  5305\n",
      "#######################################################\n",
      "Bigram Fake input unique word count :  9164\n",
      "Bigram Real input unique word count :  8589\n",
      "Bigram All unique word count :  16801\n",
      "#######################################################\n"
     ]
    }
   ],
   "source": [
    "print(\"Unigram Fake input unique word count : \",len(bow_fi_u))\n",
    "print(\"Unigram Real input unique word count : \", len(bow_ri_u))\n",
    "print(\"Unigram All unique word count : \", len(bow_all_u))\n",
    "\n",
    "print(\"#######################################################\")\n",
    "\n",
    "print(\"Bigram Fake input unique word count : \",len(bow_fi_b))\n",
    "print(\"Bigram Real input unique word count : \", len(bow_ri_b))\n",
    "print(\"Bigram All unique word count : \", len(bow_all_b))\n",
    "\n",
    "print(\"#######################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Understanding the data\n",
    "In this part I found the 3 words I asked for.Here I gave you 3 examples of real and fake  words along with statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords not use\n",
      "#######################################################\n",
      "highest 3 words reals unigram :\n",
      "trump 1484\n",
      "to 366\n",
      "donald 709\n",
      "#######################################################\n",
      "highest 3 words fake unigram :\n",
      "the 362\n",
      "trump 1130\n",
      "to 344\n",
      "#######################################################\n",
      "Stopwords use\n",
      "########################################################\n",
      "highest 3 words reals unigram :\n",
      "trump 1484 \n",
      "donald 709 \n",
      "us 186\n",
      "#######################################################\n",
      "highest 3 words fake unigram : \n",
      "trump 1130 \n",
      "donald 195 \n",
      "hillary 126\n"
     ]
    }
   ],
   "source": [
    "#highest 3 words reals unigram \n",
    "print(\"Stopwords not use\")\n",
    "print(\"#######################################################\")\n",
    "print(\"highest 3 words reals unigram :\")\n",
    "values1=list(bow_ri_u.values())\n",
    "values1.sort()\n",
    "last1=values1[-3:]\n",
    "for i in bow_ri_u:\n",
    "    if bow_ri_u[i] in last1:\n",
    "        print(i, bow_ri_u[i])\n",
    "print(\"#######################################################\")\n",
    "print(\"highest 3 words fake unigram :\")\n",
    "values2=list(bow_fi_u.values())\n",
    "values2.sort()\n",
    "last2=values2[-3:]\n",
    "for i in bow_fi_u:\n",
    "    if bow_fi_u[i] in last2:\n",
    "        print(i, bow_fi_u[i])\n",
    "print(\"#######################################################\")\n",
    "\n",
    "'''Stopword use'''\n",
    "print(\"Stopwords use\")\n",
    "print(\"########################################################\")\n",
    "print(\"highest 3 words reals unigram :\\ntrump 1484 \\ndonald 709 \\nus 186\")\n",
    "print(\"#######################################################\")\n",
    "print(\"highest 3 words fake unigram : \\ntrump 1130 \\ndonald 195 \\nhillary 126\")\n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think these words are not exactly decisive because they are so much in both datasets.So ,I gave three more examples that I thought were more decisive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>2. Implementing Naive Bayes  <br />\n",
    "In this step I tried to classify each headline in the test file algorithm as fake or real with naive bayes algorithm.  <br />\n",
    "<br>First of all, I applied this algorithm as in the example of chinese, which is described by our teacher.I used laplace smoothing because there might be words in the test file that are not in the dictionary.I first found the statistics that would be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(real) =  0.602448685631977\n",
      "P(fake) =  0.39755131436802305\n",
      "unigram total word real :  13947\n",
      "unigram total word fake :  13454\n",
      "#######################################################\n",
      "bigram total word real :  12274\n",
      "bigram total word fake :  12350\n",
      "#######################################################\n"
     ]
    }
   ],
   "source": [
    "#Naïve Bayes Learning\n",
    "#Prior Probability \n",
    "p_real =(r_line / (r_line + f_line))\n",
    "p_fake =(f_line / (r_line + f_line))\n",
    "print(\"P(real) = \",p_real)\n",
    "print(\"P(fake) = \", p_fake)\n",
    "\n",
    "#Naïve Bayes Classification\n",
    "#Likelihood\n",
    "\n",
    "total_r_uni = 0\n",
    "for r in bow_ri_u.values():\n",
    "    total_r_uni += r  \n",
    "print(\"unigram total word real : \" , total_r_uni)\n",
    "    \n",
    "total_f_uni = 0\n",
    "for f in bow_fi_u.values():\n",
    "    total_f_uni += f\n",
    "print(\"unigram total word fake : \" , total_f_uni)\n",
    "\n",
    "print(\"#######################################################\")\n",
    "\n",
    "total_r_bi = 0\n",
    "for r in bow_ri_b.values():\n",
    "    total_r_bi += r  \n",
    "print(\"bigram total word real : \" , total_r_bi)\n",
    "    \n",
    "total_f_bi = 0\n",
    "for f in bow_fi_b.values():\n",
    "    total_f_bi += f\n",
    "print(\"bigram total word fake : \" , total_f_bi)\n",
    "\n",
    "print(\"#######################################################\")\n",
    "\n",
    "#*****************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am reading the test file in the row row and I apply the following formula in the following algorithm.\n",
    "<img src =\"naivebayes.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test number:  489\n",
      "#######################################################\n",
      "Unigram true classify :  424\n",
      "Unigram false classify :  65\n",
      "#######################################################\n"
     ]
    }
   ],
   "source": [
    "p_real_dict_uni = {}\n",
    "p_fake_dict_uni = {}\n",
    "\n",
    "t_line = 0\n",
    "uni_true_classify = 0\n",
    "uni_false_classify = 0 \n",
    "val1=[]\n",
    "\n",
    "for i in range(0 , len(test_input)):\n",
    "    p_r = 1\n",
    "    p_f = 1\n",
    "    t_line += 1\n",
    "    test_line = test_input.values[i][0]\n",
    "    for word in test_line.split(\" \"):\n",
    "        #if word not in stopWords:\n",
    "            a = word\n",
    "            if a not in bow_ri_u.keys():\n",
    "                p_real_dict_uni[a] = (0 + 1)/(total_r_uni + len(bow_all_u))\n",
    "            else:\n",
    "                p_real_dict_uni[a] = (bow_ri_u[a] + 1)/(total_r_uni + len(bow_all_u)) \n",
    "                    \n",
    "            b = word\n",
    "            if b not in bow_fi_u.keys():\n",
    "                p_fake_dict_uni[b] = (0 + 1)/(total_f_uni + len(bow_all_u)) \n",
    "            else:\n",
    "                p_fake_dict_uni[b] = (bow_fi_u[b] + 1)/(total_f_uni + len(bow_all_u))\n",
    "                    \n",
    "            c = word\n",
    "            p_r  = (p_r * p_real_dict_uni[c])\n",
    "            p_f  = (p_f * p_fake_dict_uni[c])\n",
    "            \n",
    "    p_r = (p_r * p_real)\n",
    "    p_f = (p_f * p_fake)\n",
    "    \n",
    "    if p_f > p_r:\n",
    "        val1.append('fake')  \n",
    "    else:\n",
    "        val1.append('real')\n",
    "            \n",
    "#********************************************************************************\n",
    "\n",
    "print(\"total test number: \",t_line)\n",
    "\n",
    "print(\"#######################################################\")\n",
    "\n",
    "num1 =0\n",
    "for v in val1:\n",
    "    if v == test_input.values[num1][1]:\n",
    "        uni_true_classify += 1\n",
    "    else:\n",
    "        uni_false_classify += 1\n",
    "    num1 += 1\n",
    "\n",
    "print(\"Unigram true classify : \", uni_true_classify)\n",
    "print(\"Unigram false classify : \", uni_false_classify)\n",
    "\n",
    "print(\"#######################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm implementing the same algorithm for the bigram.Print results, you can see when you run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram true classify :  407\n",
      "bigram false classify :  82\n",
      "#######################################################\n"
     ]
    }
   ],
   "source": [
    "p_real_dict_bi = {}\n",
    "p_fake_dict_bi = {}\n",
    "\n",
    "val2=[]\n",
    "\n",
    "for i in range(0 , len(test_input)):\n",
    "    p_r = 1\n",
    "    p_f = 1\n",
    "    test_line = test_input.values[i][0]\n",
    "        \n",
    "    line_b = test_line.split(\" \")\n",
    "    \n",
    "    for i in range(len(line_b)-1): \n",
    "        \n",
    "        a = line_b[i],line_b[i+1]\n",
    "        if a not in bow_ri_b.keys():\n",
    "            p_real_dict_bi[a] = (0 + 1)/(total_r_bi + len(bow_all_b))\n",
    "        else:\n",
    "            p_real_dict_bi[a] = (bow_ri_b[a] + 1)/(total_r_bi + len(bow_all_b)) \n",
    "       \n",
    "        b = line_b[i],line_b[i+1]\n",
    "        if b not in bow_fi_b.keys():\n",
    "            p_fake_dict_bi[b] = (0 + 1)/(total_f_bi + len(bow_all_b))\n",
    "        else:\n",
    "            p_fake_dict_bi[b] = (bow_fi_b[b] + 1)/(total_f_bi + len(bow_all_b)) \n",
    "        \n",
    "        c = line_b[i],line_b[i+1]\n",
    "        p_r  = (p_r * p_real_dict_bi[c])\n",
    "        p_f = (p_f * p_fake_dict_bi[c])\n",
    " \n",
    "    p_r = np.log10(p_r * p_real)\n",
    "    p_f = np.log10(p_f * p_fake)\n",
    "    \n",
    "    if p_f > p_r:\n",
    "        val2.append('fake')\n",
    "    else:\n",
    "        val2.append('real')\n",
    "        \n",
    "#print(\"#######################################################\")\n",
    "\n",
    "bi_true_classify = 0\n",
    "bi_false_classify = 0\n",
    "num2 =0\n",
    "for v in val2:\n",
    "    if v == test_input.values[num2][1]:\n",
    "        bi_true_classify += 1\n",
    "    else:\n",
    "        bi_false_classify += 1\n",
    "    num2 += 1\n",
    "    \n",
    "print(\"bigram true classify : \", bi_true_classify)\n",
    "print(\"bigram false classify : \", bi_false_classify)\n",
    "\n",
    "print(\"#######################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Bigram also looked at the relationship between words, he had to make more accurate predictions, but he did not.1-2 points more accurate.I think this is due to the small size of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I calculated the accuracy.When I'm not using stopword, accuracy is like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram Accuracy :  86.70756646216769\n",
      "bigram Accuracy :  83.23108384458078\n"
     ]
    }
   ],
   "source": [
    "accuracy_uni = 100 * (uni_true_classify / t_line)\n",
    "print(\"unigram Accuracy : \",accuracy_uni)\n",
    "\n",
    "accuracy_bi = 100 * (bi_true_classify / t_line)\n",
    "print(\"bigram Accuracy : \",accuracy_bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I'm using stopword, accuracy is like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram Accuracy :  85.2760736196319\n",
      "bigram Accuracy :  83.23108384458078\n"
     ]
    }
   ],
   "source": [
    "print(\"unigram Accuracy :  85.2760736196319\")\n",
    "print(\"bigram Accuracy :  83.23108384458078\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stoword aims to filter the most commonly used words.\n",
    "Therefore, it may be useful to use it because it needs to increase accuracy.\n",
    "I have observed a few points drop accuracy  when I use the stopword.\n",
    "I don't know exactly why, but as I said earlier, we might be testing on a small data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Analyzing effect of the words on prediction\n",
    "\n",
    "In this part, I have calculated the desired words as follows.\n",
    "<img src =\"tenwords.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******Presence for real***********\n",
      "['korea', 0.986111111111111]\n",
      "['turnbull', 0.9807692307692307]\n",
      "['travel', 0.9795918367346939]\n",
      "['australia', 0.9777777777777777]\n",
      "['trumps', 0.9736842105263158]\n",
      "['climate', 0.9615384615384615]\n",
      "['north', 0.9583333333333333]\n",
      "['paris', 0.9545454545454546]\n",
      "['flynn', 0.95]\n",
      "['asia', 0.9444444444444444]\n",
      "******Presence for fake************\n",
      "['breaking', 0.9615384615384615]\n",
      "['soros', 0.95]\n",
      "['black', 0.9487179487179487]\n",
      "['reason', 0.9473684210526315]\n",
      "['woman', 0.9411764705882353]\n",
      "['steal', 0.9285714285714285]\n",
      "['interview', 0.923076923076923]\n",
      "['homeless', 0.9090909090909091]\n",
      "['12', 0.9090909090909091]\n",
      "['endingfed', 0.9090909090909091]\n",
      "****Absence for real******\n",
      "['i', 0.7959183673469389]\n",
      "['y', 0.7118133935289691]\n",
      "['c', 0.6976744186046511]\n",
      "['in', 0.6958970233306517]\n",
      "['f', 0.6934594168636723]\n",
      "['he', 0.691827173347215]\n",
      "['w', 0.6863672182821118]\n",
      "['l', 0.6794425087108011]\n",
      "['er', 0.6729475100942126]\n",
      "['the', 0.6696548632900045]\n",
      "****Absence for fake******\n",
      "['al', 0.4931506849315068]\n",
      "['na', 0.4893238434163701]\n",
      "['donald', 0.483974358974359]\n",
      "['do', 0.478538283062645]\n",
      "['on', 0.45182724252491685]\n",
      "['a', 0.4354838709677429]\n",
      "['n', 0.42574257425742584]\n",
      "['trumps', 0.4243243243243243]\n",
      "['says', 0.41169635941130905]\n",
      "['say', 0.41100702576112413]\n"
     ]
    }
   ],
   "source": [
    "print(\"################PART 3 ###################################\\n\")\n",
    "\n",
    "bag_of_word = Counter(words_counter)\n",
    "all_lines = datafake + datareal\n",
    "\n",
    "'''\n",
    "P(class) = p_real , p_fake\n",
    "P(word|class) = #of occurance of words in headlines of that class / total number of headlines of that class\n",
    "'''\n",
    "real_prior = np.log10(p_real)\n",
    "fake_prior = np.log10(p_fake)\n",
    "\n",
    "pword_bow = {}\n",
    "for w in bag_of_word.keys():\n",
    "    \n",
    "    hl_num_f = 1\n",
    "    for l in datafake:\n",
    "        if w in l:\n",
    "            hl_num_f += 1\n",
    "    w_fake = hl_num_f / len(datafake)\n",
    "    \n",
    "    hl_num_r = 1\n",
    "    for l in datareal:\n",
    "        if w in l:\n",
    "            hl_num_r += 1\n",
    "    w_real = hl_num_r / len(datareal)\n",
    "\n",
    "    pword_bow[w] = [w_real, w_fake]\n",
    "\n",
    "'''\n",
    "#PRESENCE\n",
    "\n",
    "P(class|word) = P(word|class) * P(class) / (P(word|fake)*P(fake) + P(word|real)*P(real))\n",
    "\n",
    "'''\n",
    "real_w = []\n",
    "fake_w = []\n",
    "\n",
    "for w in pword_bow.keys():\n",
    "    val = pword_bow[w]\n",
    "    real_w.append([w, val[0] * p_real / (val[0] * p_real + val[1] * p_fake)])\n",
    "    fake_w.append([w, val[1] * p_fake / (val[0] * p_real + val[1] * p_fake)])\n",
    "\n",
    "print(\"*******Presence for real***********\")\n",
    "for i in range(10):\n",
    "    print(sorted(real_w, key=lambda tup: tup[1], reverse=True)[i])\n",
    "    \n",
    "print(\"******Presence for fake************\")\n",
    "for i in range(10):\n",
    "    print(sorted(fake_w, key=lambda tup: tup[1], reverse=True)[i])\n",
    "\n",
    "'''\n",
    "#ABSENCE\n",
    "\n",
    "P(~word|class) = 1- P(word|class)\n",
    "P(~word) = headlines without word / # headlines\n",
    "\n",
    "P(class|~word) = P(~word|class)*P(class)/P(~word)\n",
    "\n",
    "'''\n",
    "real_not_w = []\n",
    "fake_not_w = []\n",
    "\n",
    "for w in pword_bow.keys():\n",
    "    headline_num = 1\n",
    "    for l in all_lines:\n",
    "        if not w in l:\n",
    "            headline_num += 1\n",
    "    not_word = headline_num / len(all_lines)\n",
    "    val = pword_bow[w]\n",
    "    fake_not_w.append([w, (1 - val[1]) * p_fake / not_word])\n",
    "    real_not_w.append([w, (1 - val[0]) * p_real / not_word])\n",
    "\n",
    "print(\"****Absence for real******\")\n",
    "for i in range(10):\n",
    "    print(sorted(real_not_w, key=lambda tup: tup[1], reverse=True)[i])\n",
    "\n",
    "print(\"****Absence for fake******\")\n",
    "for i in range(10):\n",
    "    print(sorted(fake_not_w, key=lambda tup: tup[1], reverse=True)[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especially when I use the stopword, I see that the real absences and fake presence are similar.Likekly, fake absences and real presence is similar.Because "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>****Absence for real******<br\\>\n",
    "<br>['c', 0.6976744186046511]<br\\>\n",
    "<br>['f', 0.6934594168636723]<br\\>\n",
    "<br>['w', 0.6863672182821118]<br\\>\n",
    "<br>['l', 0.6794425087108011]<br\\>\n",
    "<br>['er', 0.6729475100942126]<br\\>\n",
    "<br>['v', 0.6643109540636043]<br\\>\n",
    "<br>['b', 0.663359319631467]<br\\>\n",
    "<br>['nt', 0.6532426778242677]<br\\>\n",
    "<br>['il', 0.6519133241124943]<br\\>\n",
    "<br>['ed', 0.6496964035497431]<br\\>\n",
    "<br>****Absence for fake******<br\\>\n",
    "<br>['al', 0.4931506849315068]<br\\>\n",
    "<br>['na', 0.4893238434163701]<br\\>\n",
    "<br>['donald', 0.483974358974359]<br\\>\n",
    "<br>['n', 0.42574257425742584]<br\\>\n",
    "<br>['trumps', 0.4243243243243243]<br\\>\n",
    "<br>['says', 0.41169635941130905]<br\\>\n",
    "<br>['say', 0.41100702576112413]<br\\>\n",
    "<br>['ban', 0.40733807562710594]<br\\>\n",
    "<br>['korea', 0.40731166912850814]<br\\>\n",
    "<br>['north', 0.4065731166912851]<br\\>\n",
    "<br>*******Presence for real***********<br\\>\n",
    "<br>['korea', 0.986111111111111]<br\\>\n",
    "<br>['turnbull', 0.9807692307692307]<br\\>\n",
    "<br>['travel', 0.9795918367346939]<br\\>\n",
    "<br>['australia', 0.9777777777777777]<br\\>\n",
    "<br>['trumps', 0.9736842105263158]<br\\>\n",
    "<br>['climate', 0.9615384615384615]<br\\>\n",
    "<br>['north', 0.9583333333333333]<br\\>\n",
    "<br>['paris', 0.9545454545454546]<br\\>\n",
    "<br>['flynn', 0.95]<br\\>\n",
    "<br>['asia', 0.9444444444444444]<br\\>\n",
    "<br>******Presence for fake************<br\\>\n",
    "<br>['breaking', 0.9615384615384615]<br\\>\n",
    "<br>['soros', 0.95]<br\\>\n",
    "<br>['black', 0.9487179487179487]<br\\>\n",
    "<br>['reason', 0.9473684210526315]<br\\>\n",
    "<br>['woman', 0.9411764705882353]<br\\>\n",
    "<br>['steal', 0.9285714285714285]<br\\>\n",
    "<br>['interview', 0.923076923076923]<br\\>\n",
    "<br>['endingfed', 0.9090909090909091]<br\\>\n",
    "<br>['homeless', 0.9090909090909091]<br\\>\n",
    "<br>['12', 0.9090909090909091]<br\\>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>The result is that when I remove the words presence and make naive bayes, the result is like this.<br\\>\n",
    "\n",
    "<br>unigram Accuracy :  86.29856850715747<br\\>\n",
    "<br>bigram Accuracy :  83.23108384458078<br\\>\n",
    "    \n",
    "<br>The result is that when I remove the words absence and make naive bayes, the result is like this.<br\\>\n",
    "\n",
    "<br>unigram Accuracy :  86.29856850715747<br\\>\n",
    "<br>bigram Accuracy :  83.23108384458078<br\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This assignment best accuracy is unigram Accuracy :  86.70756646216769"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
